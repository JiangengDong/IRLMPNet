# PlaNet

## Environment

We only use 1st order differential car now.

### Car1order

* State space: [ğ‘¥, ğ‘¦, ğœƒ], invisible to the PlaNet algorithm
  * Real range: âˆ’25 < ğ‘¥ < 25, âˆ’35 < ğ‘¦ < 35, âˆ’ğœ‹ < ğœƒ < ğœ‹
  * Normalized range: âˆ’0.5 < ğ‘¥,ğ‘¦,ğœƒ <0.5
* Observation space:
  * ğ‘‚^ğ‘™: local map, 64x64x3 image consists of 0 / 1
  * ğ‘‚^ğ‘”: goal map
* Control space: ğ‘=[ğ‘£, ğœ”]
  * Real range: âˆ’1 < ğ‘£ < 2, âˆ’1 < ğœ” < 1
  * Normalized range: âˆ’0.5< ğ‘£,ğœ” <0.5
* Ground truth transition model: Integration (RK4)
* Reward function:
  * ğ‘Ÿ^ğ‘‘: distance between normalized states and normalized goal
  * ğ‘Ÿ^ğ‘: 0 if not collided, otherwise 1

## Components

This algorithm consists of four trainable models and a CEM algorithm.

* Observation encoder: convert observation to embedding
* Observation decoder: extract embedding from latent state, and reconstruct observation
* Latent transition model: a latent space transition model. When an action is given, the model predicts the next latent state; when the observation embedding is provided, the latent state is corrected accordingly
* Reward model: an approximation of the ground truth reward function. The input is the latent state.
* CEM algorithm: check Linjun's [MPC-MPNet] paper for more details.

[MPC-MPNet]: https://arxiv.org/abs/2101.06798
